{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPduteycRjWfWyrLgfJW8g/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/altn714/ESAA/blob/main/13%EC%A3%BC%EC%B0%A8_%EA%B3%BC%EC%A0%9C_p319~341.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "편향-분산 트레이드오프\n",
        "- 고편향: 지나치게 한 방향으로 치우침\n",
        "- 고분산: 학습 데이터 하나하나의 특성 반영 -> 매우 복잡한 모델, 높은 변동성\n",
        "- 일반적으로 편향과 분산은 한쪽이 높으면 한 쪽이 낮아지는 경향, 즉 편향이 높으면 분산은 낮아지고 과소적합, 반대로 분산이 높으면 편향잉 낮아진다. 과적합, 다음 그림은 편향과 분산의 관계에 따른 전체 오류 값의 변화를 보여줌. 편향이 너무 높으면 전체 오류가 높습니다. 편향을 점점 낮추면 동시에 분산이 높아지고 전체 오류도 낮아지게 됨. 편향을 낮추고 분산을 높이면서 전체 오류가 가장 낮아지는 골디락스 지점을 통과하면서 분산을 지속적으로 높이면 전체 오류 값이 오히려 증가하면서 예측 성능이 다시 저하됨. 높은 편향 낮은 분산에서 과소적합 되기 쉬우며 낮은 편향 높은 분산에서 과적합 되기 쉬움. 편향과 분산이 서로 트레이드 오프를 이루면서 오류 cost 값이 최대로 낮아지는 모델을 구축하는 것이 가장 효율적인 머신러닝 예측 모델을 만드는 방법."
      ],
      "metadata": {
        "id": "eLU34NhjFKUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "규제선형 모델 개요\n",
        "좋은 머신러닝 회귀 모델의 특징? 지나치게 예측 곡선을 단순화해 데이터에 적합하지 않는 과소적합 모델이 만들어짐. 반대로 모든 데이터에 적합한 회귀식을 만들기 위해 다항식이 복잡해지고 회귀계수가 매우 크게 설정이 되면서 평가 데이터 세트에 대해서는 형편없는 예측 성능을 보였음. 따라서 회귀 모델은 적절히 데이터에 적합하면서도 회귀계수가 기하급수적으로 커지는 것을 제어할 수 있어야함. 이전까자ㅣ 선형 모델의 비용함수는 RSS를 최소하하는 즉 실젯값과 예측 값의 차이를 최소화 하는 것을 고려했음. 그러다 보니 학습 데이터에 지나치게 맞추게 되고, 회귀계수 값이 커졌습니다. 이럴 경우 변동성이 오히려 심해져서 테스트 데이터 세트에서는 예측 성능이 저하되기 쉬움. 이를 반영해 비용함수는 학습 데이터의 잔차 오류 값을 최소로 하는 RSS 최소화 방법과 과적합을 방지하기 위해 회귀 계수 값이 커지지 않도록 하는 방법이 서로 균형을 이뤄야함. \n",
        "\n",
        "> 정리\n",
        "최적 모델을 위한 cost 함수 구성요소 = 학습데이터 잔차 오류 최소화 + 회귀계수 크기 제어\n",
        "\n",
        "이렇게 회귀 계수의 크기를 제어해 과적합을 개선하려면 비용 함수의 목표는 \n",
        "- 비용함수 목표 = Min(RSS(W) + alpha*||W||22)\n",
        "\n",
        "여기서 alpaha는 학습 데이터 적합 정도 (회귀계수 크기) 제어를 수행하는 튜닝 파라미터임. -> 과적합 방지용 / 비용함수의 목표를 최소화하는 W 벡터를 찾는 것일때, alpha가 어떤 역할을 하는지 살펴보겠습니다. alpha가 0이라면 비용함수 식은 기존과 동일한 Min(RSS) 일 것입니다. 반면에 alpha가 무한대면 비용함수의 식은 RSS(W)에 비해 alpha*||W||22 값이 너무 커지게 되므로 W값을 0으로 작게 만들어야 cost가 최소화되는 비용함수 목표를 달성할 수 있습니다. 즉, alpha값을 크게하면 비용함수는 회귀계수 W의 값을 작게 해 과적합을 개선할 수 있으며, alpha 값을 작게 하면 회귀계수 W의 값이 커져도 어느 정도 상쇄가 가능하므로 학습 데이터 적합을 더 개선할 수 있습니다.\n",
        "\n",
        "즉, alpha를 0에서부터 지속적으로 값을 증가시키면 회귀계수 값의 크기를 감소시킬 수 있습니다. 이처럼 비용 함수에 alpha 값으로 페널티를 부여해 회귀계수 값의 크기를 감소시켜 과적합을 개선하는 방식을 규제라고 부른다. 규제의 L2 방식ㅇ은 위에서 설명한 바와 같이 W의 제곱에 alpha라는 페널티를 부여하는 방식이고 이러한 L2 규제를 적용한 회귀를 릿지 회귀라고 함. 라쏘 회귀는 L1 규제를 적용한 회귀이다. L1 규제는 alpha*||W||1과 같이 W의 절댓값에 대해 페널티를 부여합니다. L1규제를 적용하면 영향력이 크지 않은 회귀계수 값을 0으로 변환함.\n",
        "\n",
        "- 릿지 회귀: 사이킷런은 Ridge 클래스를 통해 릿지 회귀를 구현한다. Ridge클래스의 주요 생성 파라미터는 alpha이며, 이는 릿지 회귀의 alpha L2 규제 계수에 해당한다. 앞 예제의 보스턴 주택 가격을 Ridge 클래스를 이용해 다시 예측하고, 예측 성능을 cross_val_score()로 평가해보겠습니다. 앞의 Linear Regression예제에서 사용한 데이터 세트인 X_data, 그리고 target 데이터 세트인 y_target을 그대로 이용."
      ],
      "metadata": {
        "id": "Ewk1ntVOHJ0I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42IvSvTeE8UF",
        "outputId": "236dbd4f-ff1b-46e3-da45-d8f83f367913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boston 데이터 세트 크기: (506, 14)\n"
          ]
        }
      ],
      "source": [
        "#전 과제 데이터 로드\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.datasets import load_boston\n",
        "%matplotlib Inline\n",
        "\n",
        "#boston 데이터 세트 로드 \n",
        "boston = load_boston()\n",
        "\n",
        "#boston 데이터 세트 DataFrame 변환\n",
        "bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
        "\n",
        "#boston 데이터 세트의 target 배열은 주택 가격임. 이를 PRICE 칼럼으로 DataFrame에 추가\n",
        "bostonDF['PRICE'] = boston.target\n",
        "print(\"Boston 데이터 세트 크기:\", bostonDF.shape)\n",
        "bostonDF.head()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score \n",
        "#메트릭스 모듈의 mean_squared_error()과 r2_score() API 이용\n",
        "\n",
        "y_target = bostonDF['PRICE']\n",
        "X_data = bostonDF.drop(['PRICE'],axis=1, inplace=False)\n",
        "X_train, X_test, y_train,y_test = train_test_split(X_data, y_target,test_size=0.3, random_state=156)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#alpha=10으로 설정해 릿지 회귀 수행\n",
        "ridge = Ridge(alpha=10)\n",
        "neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring = \"neg_mean_squared_error\",cv=5)\n",
        "rmse_scores = np.sqrt(-1*neg_mse_scores)\n",
        "avg_rmse = np.mean(rmse_scores)"
      ],
      "metadata": {
        "id": "jyX-dLGUR5qC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('5 folds의 개별 Negative MSE scores:', np.round(neg_mse_scores,3))\n",
        "print('5 folds의 개별 RMSE scores:', np.round(rmse_scores,3))\n",
        "print('5 folds의 평균 RMSE:{0.3f}', format(avg_rmse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "463TmXsPTjHM",
        "outputId": "dea50f6b-74ec-49ec-faff-0390b5cc8505"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 folds의 개별 Negative MSE scores: [-11.422 -24.294 -28.144 -74.599 -28.517]\n",
            "5 folds의 개별 RMSE scores: [3.38  4.929 5.305 8.637 5.34 ]\n",
            "5 folds의 평균 RMSE:{0.3f} 5.518166280868968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "릿지의 5개 폴드 세트의 평균 RMSE는 5.524입니다. 앞 예제의 규제가 없는 linear regression의 RMSE 평균인 5.836보다 더 뛰어난 예측 성능을 보여줍니다.\n",
        "이번에는 릿지의 alpha 값을 0,0.1,1,10,100 으로 변화 시키면서  RMSE와 회귀계수 값의 변화를 살펴보겠습니다. alpha 값을 변화하면서 RMSE 값과 각 피처의 회귀계수를 시각화하고 DataFrame에 저장하는 예제입니다. 예제의 결과에서 보겠지만, 릿지 회귀는 alpha 값이 커질수록 회귀계수 값을 작게 만듭니다. 먼저 alpha 값의 변화에 따른 값의 변화에 따른 5폴드의 RMSE 평균값을 변환하는 코드부터 작성"
      ],
      "metadata": {
        "id": "XUSZ7K5uT9_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#릿지에 사용될 alpha 파라미터 값을 정의\n",
        "alphas=[0,0.1,1,10,100]\n",
        "\n",
        "#alpha list값을 반복하면서 alpha에 따른 평균 rmse를 구함.\n",
        "for alpha in alphas:\n",
        "  ridge = Ridge(alpha = alpha)\n",
        "\n",
        "  #cross_val_score를 이용해 5 폴드의 평균 RMSE 계산\n",
        "  neg_mse_scores = cross_val_score(ridge,X_data,y_target, scoring=\"neg_mean_squared_error\",cv=5)\n",
        "  avg_rmse = np.mean(np.sqrt(-1*neg_mse_scores))\n",
        "  print('alpha {0}일 때 5 folds의 평균 RMSE:{1:.3f}'.format(alpha,avg_rmse))\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjzKSBAkUiwP",
        "outputId": "a99129c4-2fa8-44e1-9e18-537bdf80c28b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alpha 0일 때 5 folds의 평균 RMSE:5.829\n",
            "alpha 0.1일 때 5 folds의 평균 RMSE:5.788\n",
            "alpha 1일 때 5 folds의 평균 RMSE:5.653\n",
            "alpha 10일 때 5 folds의 평균 RMSE:5.518\n",
            "alpha 100일 때 5 folds의 평균 RMSE:5.330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "alpha가 100일 때 평균 RMSE가 5.330으로 가장 좋습니다. 가장 낮은 값 이번에는 alpha 값의 변화에 따른 피처의 회귀계수 값을 가로 막대 그래프로 시각화하겠습니다. 회귀계수를 Ridge 객체의 coef_속성에 추출한 뒤, Series 객체로 만들어서 시본 가로 막대 차트로 표시하고, DataFrame에 alpha 값 별 회귀 계수로 지정"
      ],
      "metadata": {
        "id": "aCwLYQOOVV8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#각 alpha에 따른 회귀 계수 값을 시각화하기 위해 5개의 열로 된 맷플롯립 축 생성\n",
        "fig, axs = plt.subplots(figsize=(8,16), nrows=1, ncols=5)\n",
        "#각 alpha에 따른 회귀 계수 값을 데이터로 저장하기 위한 DF 생성\n",
        "coeff_df = pd.DataFrame()\n",
        "#alphas 리스트 값을 차례대로 입력해 회귀 계수 값 시각화 및 데이터 저장. pos는 axis 위치 지정\n",
        "for pos, alpha in enumerate(alphas):\n",
        "  ridge = Ridge(alpha=alpha)\n",
        "  ridge.fit(X_data,y_target)\n",
        "  #alpha에 따른 피처별로 회귀 계수를 Series로 변환하고 이를 DataFrame의 칼럼으로 추가.  \n",
        "  coeff = pd.Series(data=ridge.coef_, index = X_data.columns)\n",
        "  colname = 'alpha:'+str(alpha)\n",
        "  coeff_df[colname]"
      ],
      "metadata": {
        "id": "igUVgtlsVp5C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}